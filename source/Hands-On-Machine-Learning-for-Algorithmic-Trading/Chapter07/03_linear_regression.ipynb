{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction using the Quantopian Trading Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook linear_regression.ipynb contains examples for the prediction of stock prices using OLS with statsmodels and sklearn, as well as ridge and lasso models. \n",
    "\n",
    "It is designed to run as a notebook on the Quantopian research platform and relies on the factor_library introduced in Chapter 4, Research and Evaluation of Alpha Factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written for the Quantopian [research environment](https://www.quantopian.com/research). You can upload it after signing up and execute it on the Quantopian platform to gain access to the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import talib\n",
    "import re\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from quantopian.research import run_pipeline\n",
    "from quantopian.pipeline import Pipeline, factors, filters, classifiers\n",
    "from quantopian.pipeline.data.builtin import USEquityPricing\n",
    "from quantopian.pipeline.factors import (Latest, \n",
    "                                         Returns, \n",
    "                                         AverageDollarVolume, \n",
    "                                         SimpleMovingAverage,\n",
    "                                         EWMA,\n",
    "                                         BollingerBands,\n",
    "                                         CustomFactor,\n",
    "                                         MarketCap,\n",
    "                                        SimpleBeta)\n",
    "from quantopian.pipeline.filters import QTradableStocksUS, StaticAssets\n",
    "from quantopian.pipeline.data.quandl import fred_usdontd156n as libor\n",
    "from empyrical import max_drawdown, sortino_ratio\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Fundamentals #\n",
    "################\n",
    "\n",
    "# Morningstar fundamentals (2002 - Ongoing)\n",
    "# https://www.quantopian.com/help/fundamentals\n",
    "from quantopian.pipeline.data import Fundamentals\n",
    "\n",
    "#####################\n",
    "# Analyst Estimates #\n",
    "#####################\n",
    "\n",
    "# Earnings Surprises - Zacks (27 May 2006 - Ongoing)\n",
    "# https://www.quantopian.com/data/zacks/earnings_surprises\n",
    "from quantopian.pipeline.data.zacks import EarningsSurprises\n",
    "from quantopian.pipeline.factors.zacks import BusinessDaysSinceEarningsSurprisesAnnouncement\n",
    "\n",
    "##########\n",
    "# Events #\n",
    "##########\n",
    "\n",
    "# Buyback Announcements - EventVestor (01 Jun 2007 - Ongoing)\n",
    "# https://www.quantopian.com/data/eventvestor/buyback_auth\n",
    "from quantopian.pipeline.data.eventvestor import BuybackAuthorizations\n",
    "from quantopian.pipeline.factors.eventvestor import BusinessDaysSinceBuybackAuth\n",
    "\n",
    "# CEO Changes - EventVestor (01 Jan 2007 - Ongoing)\n",
    "# https://www.quantopian.com/data/eventvestor/ceo_change\n",
    "from quantopian.pipeline.data.eventvestor import CEOChangeAnnouncements\n",
    "\n",
    "# Dividends - EventVestor (01 Jan 2007 - Ongoing)\n",
    "# https://www.quantopian.com/data/eventvestor/dividends\n",
    "from quantopian.pipeline.data.eventvestor import (\n",
    "    DividendsByExDate,\n",
    "    DividendsByPayDate,\n",
    "    DividendsByAnnouncementDate,\n",
    ")\n",
    "from quantopian.pipeline.factors.eventvestor import (\n",
    "    BusinessDaysSincePreviousExDate,\n",
    "    BusinessDaysUntilNextExDate,\n",
    "    BusinessDaysSinceDividendAnnouncement,\n",
    ")\n",
    "\n",
    "# Earnings Calendar - EventVestor (01 Jan 2007 - Ongoing)\n",
    "# https://www.quantopian.com/data/eventvestor/earnings_calendar\n",
    "from quantopian.pipeline.data.eventvestor import EarningsCalendar\n",
    "from quantopian.pipeline.factors.eventvestor import (\n",
    "    BusinessDaysUntilNextEarnings,\n",
    "    BusinessDaysSincePreviousEarnings\n",
    ")\n",
    "\n",
    "# 13D Filings - EventVestor (01 Jan 2007 - Ongoing)\n",
    "# https://www.quantopian.com/data/eventvestor/_13d_filings\n",
    "from quantopian.pipeline.data.eventvestor import _13DFilings\n",
    "from quantopian.pipeline.factors.eventvestor import BusinessDaysSince13DFilingsDate\n",
    "\n",
    "#############\n",
    "# Sentiment #\n",
    "#############\n",
    "\n",
    "# News Sentiment - Sentdex Sentiment Analysis (15 Oct 2012 - Ongoing)\n",
    "# https://www.quantopian.com/data/sentdex/sentiment\n",
    "from quantopian.pipeline.data.sentdex import sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select a universe of equities and a time horizon, build and transform alpha factors that we will use as features, calculate forward returns that we aim to predict, and potentially clean our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading days per period\n",
    "MONTH = 21\n",
    "YEAR = 12 * MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '2014-01-01'\n",
    "END = '2015-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use equity data for the years 2014 and 2015 from a custom Q100US universe that uses built-in filters, factors, and classifiers to select the 100 stocks with the highest average dollar volume of the last 200 trading days filtered by additional default criteria (see Quantopian docs linked on GitHub for detail). The universe dynamically updates based on the filter criteria so that, while there are 100 stocks at any given point, there may be more than 100 distinct equities in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q100US():\n",
    "    return filters.make_us_equity_universe(\n",
    "        target_size=100,\n",
    "        rankby=factors.AverageDollarVolume(window_length=200),\n",
    "        mask=filters.default_us_equity_universe_mask(),\n",
    "        groupby=classifiers.fundamentals.Sector(),\n",
    "        max_group_weight=0.3,\n",
    "        smoothing_func=lambda f: f.downsample('month_start'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVERSE = StaticAssets(symbols(['MSFT', 'AAPL']))\n",
    "UNIVERSE = Q100US()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnualizedData(CustomFactor):\n",
    "    # Get the sum of the last 4 reported values\n",
    "    window_length = 260\n",
    "\n",
    "    def compute(self, today, assets, out, asof_date, values):\n",
    "        for asset in range(len(assets)):\n",
    "            # unique asof dates indicate availability of new figures\n",
    "            _, filing_dates = np.unique(asof_date[:, asset], return_index=True)\n",
    "            quarterly_values = values[filing_dates[-4:], asset]\n",
    "            # ignore annual windows with <4 quarterly data points\n",
    "            if len(~np.isnan(quarterly_values)) != 4:\n",
    "                out[asset] = np.nan\n",
    "            else:\n",
    "                out[asset] = np.sum(quarterly_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnualAvg(CustomFactor):\n",
    "    window_length = 252\n",
    "    \n",
    "    def compute(self, today, assets, out, values):\n",
    "        out[:] = (values[0] + values[-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_pipeline(factors):\n",
    "    start = time()\n",
    "    pipe = Pipeline({k: v(mask=UNIVERSE).rank() for k, v in factors.items()},\n",
    "                    screen=UNIVERSE)\n",
    "    result = run_pipeline(pipe, start_date=START, end_date=END)\n",
    "    return result, time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFactors:\n",
    "    \"\"\"Definitions of factors for cross-sectional trading algorithms\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def PriceToSalesTTM(**kwargs):\n",
    "        \"\"\"Last closing price divided by sales per share\"\"\"        \n",
    "        return Fundamentals.ps_ratio.latest\n",
    "\n",
    "    @staticmethod\n",
    "    def PriceToEarningsTTM(**kwargs):\n",
    "        \"\"\"Closing price divided by earnings per share (EPS)\"\"\"\n",
    "        return Fundamentals.pe_ratio.latest\n",
    " \n",
    "    @staticmethod\n",
    "    def PriceToDilutedEarningsTTM(mask):\n",
    "        \"\"\"Closing price divided by diluted EPS\"\"\"\n",
    "        last_close = USEquityPricing.close.latest\n",
    "        diluted_eps = AnnualizedData(inputs = [Fundamentals.diluted_eps_earnings_reports_asof_date,\n",
    "                                               Fundamentals.diluted_eps_earnings_reports],\n",
    "                                     mask=mask)\n",
    "        return last_close / diluted_eps\n",
    "\n",
    "    @staticmethod\n",
    "    def PriceToForwardEarnings(**kwargs):\n",
    "        \"\"\"Price to Forward Earnings\"\"\"\n",
    "        return Fundamentals.forward_pe_ratio.latest\n",
    "    \n",
    "    @staticmethod\n",
    "    def DividendYield(**kwargs):\n",
    "        \"\"\"Dividends per share divided by closing price\"\"\"\n",
    "        return Fundamentals.trailing_dividend_yield.latest\n",
    "\n",
    "    @staticmethod\n",
    "    def PriceToFCF(mask):\n",
    "        \"\"\"Price to Free Cash Flow\"\"\"\n",
    "        last_close = USEquityPricing.close.latest\n",
    "        fcf_share = AnnualizedData(inputs = [Fundamentals.fcf_per_share_asof_date,\n",
    "                                             Fundamentals.fcf_per_share],\n",
    "                                   mask=mask)\n",
    "        return last_close / fcf_share\n",
    "\n",
    "    @staticmethod\n",
    "    def PriceToOperatingCashflow(mask):\n",
    "        \"\"\"Last Close divided by Operating Cash Flows\"\"\"\n",
    "        last_close = USEquityPricing.close.latest\n",
    "        cfo_per_share = AnnualizedData(inputs = [Fundamentals.cfo_per_share_asof_date,\n",
    "                                                 Fundamentals.cfo_per_share],\n",
    "                                       mask=mask)        \n",
    "        return last_close / cfo_per_share\n",
    "\n",
    "    @staticmethod\n",
    "    def PriceToBook(mask):\n",
    "        \"\"\"Closing price divided by book value\"\"\"\n",
    "        last_close = USEquityPricing.close.latest\n",
    "        book_value_per_share = AnnualizedData(inputs = [Fundamentals.book_value_per_share_asof_date,\n",
    "                                              Fundamentals.book_value_per_share],\n",
    "                                             mask=mask)        \n",
    "        return last_close / book_value_per_share\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def EVToFCF(mask):\n",
    "        \"\"\"Enterprise Value divided by Free Cash Flows\"\"\"\n",
    "        fcf = AnnualizedData(inputs = [Fundamentals.free_cash_flow_asof_date,\n",
    "                                       Fundamentals.free_cash_flow],\n",
    "                             mask=mask)\n",
    "        return Fundamentals.enterprise_value.latest / fcf\n",
    "\n",
    "    @staticmethod\n",
    "    def EVToEBITDA(mask):\n",
    "        \"\"\"Enterprise Value to Earnings Before Interest, Taxes, Deprecation and Amortization (EBITDA)\"\"\"\n",
    "        ebitda = AnnualizedData(inputs = [Fundamentals.ebitda_asof_date,\n",
    "                                          Fundamentals.ebitda],\n",
    "                                mask=mask)\n",
    "\n",
    "        return Fundamentals.enterprise_value.latest / ebitda\n",
    "\n",
    "    @staticmethod\n",
    "    def EBITDAYield(mask):\n",
    "        \"\"\"EBITDA divided by latest close\"\"\"\n",
    "        ebitda = AnnualizedData(inputs = [Fundamentals.ebitda_asof_date,\n",
    "                                          Fundamentals.ebitda],\n",
    "                                mask=mask)\n",
    "        return USEquityPricing.close.latest / ebitda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_FACTORS = {\n",
    "    'DividendYield'            : ValueFactors.DividendYield,\n",
    "    'EBITDAYield'              : ValueFactors.EBITDAYield,\n",
    "    'EVToEBITDA'               : ValueFactors.EVToEBITDA,\n",
    "    'EVToFCF'                  : ValueFactors.EVToFCF,\n",
    "    'PriceToBook'              : ValueFactors.PriceToBook,\n",
    "    'PriceToDilutedEarningsTTM': ValueFactors.PriceToDilutedEarningsTTM,\n",
    "    'PriceToEarningsTTM'       : ValueFactors.PriceToEarningsTTM,\n",
    "    'PriceToFCF'               : ValueFactors.PriceToFCF,\n",
    "    'PriceToForwardEarnings'   : ValueFactors.PriceToForwardEarnings,\n",
    "    'PriceToOperatingCashflow' : ValueFactors.PriceToOperatingCashflow,\n",
    "    'PriceToSalesTTM'          : ValueFactors.PriceToSalesTTM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_factors, t = factor_pipeline(VALUE_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "value_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumFactors:\n",
    "    \"\"\"Custom Momentum Factors\"\"\"\n",
    "    class PercentAboveLow(CustomFactor):\n",
    "        \"\"\"Percentage of current close above low \n",
    "        in lookback window of window_length days\n",
    "        \"\"\"\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "            out[:] = close[-1] / np.min(close, axis=0) - 1\n",
    "\n",
    "    class PercentBelowHigh(CustomFactor):\n",
    "        \"\"\"Percentage of current close below high \n",
    "        in lookback window of window_length days\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "            \n",
    "        def compute(self, today, assets, out, close):\n",
    "            out[:] = close[-1] / np.max(close, axis=0) - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dx(timeperiod=14):\n",
    "        class DX(CustomFactor):\n",
    "            \"\"\"Directional Movement Index\"\"\"\n",
    "            inputs = [USEquityPricing.high, \n",
    "                      USEquityPricing.low, \n",
    "                      USEquityPricing.close]\n",
    "            window_length = timeperiod + 1\n",
    "            \n",
    "            def compute(self, today, assets, out, high, low, close):\n",
    "                out[:] = [talib.DX(high[:, i], \n",
    "                                   low[:, i], \n",
    "                                   close[:, i], \n",
    "                                   timeperiod=timeperiod)[-1] \n",
    "                          for i in range(len(assets))]\n",
    "        return DX  \n",
    "\n",
    "    @staticmethod\n",
    "    def make_mfi(timeperiod=14):\n",
    "        class MFI(CustomFactor):\n",
    "            \"\"\"Money Flow Index\"\"\"\n",
    "            inputs = [USEquityPricing.high, \n",
    "                      USEquityPricing.low, \n",
    "                      USEquityPricing.close,\n",
    "                      USEquityPricing.volume]\n",
    "            window_length = timeperiod + 1\n",
    "            \n",
    "            def compute(self, today, assets, out, high, low, close, vol):\n",
    "                out[:] = [talib.MFI(high[:, i], \n",
    "                                    low[:, i], \n",
    "                                    close[:, i],\n",
    "                                    vol[:, i],\n",
    "                                    timeperiod=timeperiod)[-1] \n",
    "                          for i in range(len(assets))]\n",
    "        return MFI           \n",
    "\n",
    "    @staticmethod\n",
    "    def make_oscillator(fastperiod=12, slowperiod=26, matype=0):\n",
    "        class PPO(CustomFactor):\n",
    "            \"\"\"12/26-Day Percent Price Oscillator\"\"\"\n",
    "            inputs = [USEquityPricing.close]\n",
    "            window_length = slowperiod\n",
    "\n",
    "            def compute(self, today, assets, out, close_prices):\n",
    "                out[:] = [talib.PPO(close,\n",
    "                                    fastperiod=fastperiod,\n",
    "                                    slowperiod=slowperiod, \n",
    "                                    matype=matype)[-1]\n",
    "                         for close in close_prices.T]\n",
    "        return PPO\n",
    "\n",
    "    @staticmethod\n",
    "    def make_stochastic_oscillator(fastk_period=5, slowk_period=3, slowd_period=3, \n",
    "                                   slowk_matype=0, slowd_matype=0):                \n",
    "        class StochasticOscillator(CustomFactor):\n",
    "            \"\"\"20-day Stochastic Oscillator \"\"\"\n",
    "            inputs = [USEquityPricing.high, \n",
    "                      USEquityPricing.low, \n",
    "                      USEquityPricing.close]\n",
    "            outputs = ['slowk', 'slowd']\n",
    "            window_length = fastk_period * 2\n",
    "            \n",
    "            def compute(self, today, assets, out, high, low, close):\n",
    "                slowk, slowd = [talib.STOCH(high[:, i],\n",
    "                                            low[:, i],\n",
    "                                            close[:, i], \n",
    "                                            fastk_period=fastk_period,\n",
    "                                            slowk_period=slowk_period, \n",
    "                                            slowk_matype=slowk_matype, \n",
    "                                            slowd_period=slowd_period, \n",
    "                                            slowd_matype=slowd_matype)[-1] \n",
    "                                for i in range(len(assets))]\n",
    "\n",
    "                out.slowk[:], out.slowd[:] = slowk[-1], slowd[-1]\n",
    "        return StochasticOscillator\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_trendline(timeperiod=252):                \n",
    "        class Trendline(CustomFactor):\n",
    "            inputs = [USEquityPricing.close]\n",
    "            \"\"\"52-Week Trendline\"\"\"\n",
    "            window_length = timeperiod\n",
    "\n",
    "            def compute(self, today, assets, out, close_prices):\n",
    "                out[:] = [talib.LINEARREG_SLOPE(close, \n",
    "                                   timeperiod=timeperiod)[-1] \n",
    "                          for close in close_prices.T]\n",
    "        return Trendline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOMENTUM_FACTORS = {\n",
    "    'Percent Above Low'            : MomentumFactors.PercentAboveLow,\n",
    "    'Percent Below High'           : MomentumFactors.PercentBelowHigh,\n",
    "    'Price Oscillator'             : MomentumFactors.make_oscillator(),\n",
    "    'Money Flow Index'             : MomentumFactors.make_mfi(),\n",
    "    'Directional Movement Index'   : MomentumFactors.make_dx(),\n",
    "    'Trendline'                    : MomentumFactors.make_trendline()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_factors, t = factor_pipeline(MOMENTUM_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "momentum_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficiencyFactors:\n",
    "\n",
    "    @staticmethod\n",
    "    def CapexToAssets(mask):\n",
    "        \"\"\"Capital Expenditure divided by Total Assets\"\"\"\n",
    "        capex = AnnualizedData(inputs = [Fundamentals.capital_expenditure_asof_date,\n",
    "                                         Fundamentals.capital_expenditure],\n",
    "                                     mask=mask)   \n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return - capex / assets\n",
    "\n",
    "    @staticmethod\n",
    "    def CapexToSales(mask):\n",
    "        \"\"\"Capital Expenditure divided by Total Revenue\"\"\"\n",
    "        capex = AnnualizedData(inputs = [Fundamentals.capital_expenditure_asof_date,\n",
    "                                         Fundamentals.capital_expenditure],\n",
    "                                     mask=mask)   \n",
    "        revenue = AnnualizedData(inputs = [Fundamentals.total_revenue_asof_date,\n",
    "                                         Fundamentals.total_revenue],\n",
    "                                     mask=mask)         \n",
    "        return - capex / revenue\n",
    "  \n",
    "    @staticmethod\n",
    "    def CapexToFCF(mask):\n",
    "        \"\"\"Capital Expenditure divided by Free Cash Flows\"\"\"\n",
    "        capex = AnnualizedData(inputs = [Fundamentals.capital_expenditure_asof_date,\n",
    "                                         Fundamentals.capital_expenditure],\n",
    "                                     mask=mask)   \n",
    "        free_cash_flow = AnnualizedData(inputs = [Fundamentals.free_cash_flow_asof_date,\n",
    "                                         Fundamentals.free_cash_flow],\n",
    "                                     mask=mask)         \n",
    "        return - capex / free_cash_flow\n",
    "\n",
    "    @staticmethod\n",
    "    def EBITToAssets(mask):\n",
    "        \"\"\"Earnings Before Interest and Taxes (EBIT) divided by Total Assets\"\"\"\n",
    "        ebit = AnnualizedData(inputs = [Fundamentals.ebit_asof_date,\n",
    "                                         Fundamentals.ebit],\n",
    "                                     mask=mask)   \n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return ebit / assets\n",
    "    \n",
    "    @staticmethod\n",
    "    def CFOToAssets(mask):\n",
    "        \"\"\"Operating Cash Flows divided by Total Assets\"\"\"\n",
    "        cfo = AnnualizedData(inputs = [Fundamentals.operating_cash_flow_asof_date,\n",
    "                                         Fundamentals.operating_cash_flow],\n",
    "                                     mask=mask)   \n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return cfo / assets \n",
    "    \n",
    "    @staticmethod\n",
    "    def RetainedEarningsToAssets(mask):\n",
    "        \"\"\"Retained Earnings divided by Total Assets\"\"\"\n",
    "        retained_earnings = AnnualizedData(inputs = [Fundamentals.retained_earnings_asof_date,\n",
    "                                         Fundamentals.retained_earnings],\n",
    "                                     mask=mask)   \n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return retained_earnings / assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EFFICIENCY_FACTORS = {\n",
    "    'CFO To Assets' :EfficiencyFactors.CFOToAssets,\n",
    "    'Capex To Assets' :EfficiencyFactors.CapexToAssets,\n",
    "    'Capex To FCF' :EfficiencyFactors.CapexToFCF,\n",
    "    'Capex To Sales' :EfficiencyFactors.CapexToSales,\n",
    "    'EBIT To Assets' :EfficiencyFactors.EBITToAssets,\n",
    "    'Retained Earnings To Assets' :EfficiencyFactors.RetainedEarningsToAssets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_factors, t = factor_pipeline(EFFICIENCY_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "efficiency_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskFactors:\n",
    "\n",
    "    @staticmethod\n",
    "    def LogMarketCap(mask):\n",
    "        \"\"\"Log of Market Capitalization log(Close Price * Shares Outstanding)\"\"\"\n",
    "        return np.log(MarketCap(mask=mask))\n",
    " \n",
    "    class DownsideRisk(CustomFactor):\n",
    "        \"\"\"Mean returns divided by std of 1yr daily losses (Sortino Ratio)\"\"\"\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "            ret = pd.DataFrame(close).pct_change()\n",
    "            out[:] = ret.mean().div(ret.where(ret<0).std())\n",
    "\n",
    "    @staticmethod\n",
    "    def MarketBeta(**kwargs):\n",
    "        \"\"\"Slope of 1-yr regression of price returns against index returns\"\"\"\n",
    "        return SimpleBeta(target=symbols('SPY'), regression_length=252) \n",
    "\n",
    "    class DownsideBeta(CustomFactor):\n",
    "        \"\"\"Slope of 1yr regression of returns on negative index returns\"\"\"\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "            t = len(close)\n",
    "            assets = pd.DataFrame(close).pct_change()\n",
    "            \n",
    "            start_date = (today - pd.DateOffset(years=1)).strftime('%Y-%m-%d')\n",
    "            spy = get_pricing('SPY', \n",
    "                              start_date=start_date, \n",
    "                              end_date=today.strftime('%Y-%m-%d')).reset_index(drop=True)\n",
    "            spy_neg_ret = (spy\n",
    "                           .close_price\n",
    "                           .iloc[-t:]\n",
    "                           .pct_change()\n",
    "                           .pipe(lambda x: x.where(x<0)))\n",
    "    \n",
    "            out[:] = assets.apply(lambda x: x.cov(spy_neg_ret)).div(spy_neg_ret.var())         \n",
    "\n",
    "    class Vol3M(CustomFactor):\n",
    "        \"\"\"3-month Volatility: Standard deviation of returns over 3 months\"\"\"\n",
    "\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 63\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "            out[:] = np.log1p(pd.DataFrame(close).pct_change()).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RISK_FACTORS = {\n",
    "    'Log Market Cap' : RiskFactors.LogMarketCap,\n",
    "    'Downside Risk'  : RiskFactors.DownsideRisk,\n",
    "    'Index Beta'     : RiskFactors.MarketBeta,\n",
    "#     'Downside Beta'  : RiskFactors.DownsideBeta,    \n",
    "    'Volatility 3M'  : RiskFactors.Vol3M,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors, t = factor_pipeline(RISK_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "risk_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_pipeline():\n",
    "    revenue = AnnualizedData(inputs = [Fundamentals.total_revenue_asof_date,\n",
    "                                       Fundamentals.total_revenue],\n",
    "                             mask=UNIVERSE)\n",
    "    eps = AnnualizedData(inputs = [Fundamentals.diluted_eps_earnings_reports_asof_date,\n",
    "                                       Fundamentals.diluted_eps_earnings_reports],\n",
    "                             mask=UNIVERSE)    \n",
    "\n",
    "    return Pipeline({'Sales': revenue,\n",
    "                     'EPS': eps,\n",
    "                     'Total Assets': Fundamentals.total_assets.latest,\n",
    "                     'Net Debt': Fundamentals.net_debt.latest},\n",
    "                    screen=UNIVERSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer = time()\n",
    "growth_factors = run_pipeline(growth_pipeline(), start_date=START, end_date=END)\n",
    "\n",
    "for col in growth_result.columns:\n",
    "    for month in [3, 12]:\n",
    "        new_col = col + ' Growth {}M'.format(month)\n",
    "        kwargs = {new_col: growth_factors[col].pct_change(month*MONTH).groupby(level=1).rank()}        \n",
    "        growth_factors = growth_factors.assign(**kwargs)\n",
    "print('Pipeline run time {:.2f} secs'.format(time() - start_timer))\n",
    "growth_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityFactors:\n",
    "    \n",
    "    @staticmethod\n",
    "    def AssetTurnover(mask):\n",
    "        \"\"\"Sales divided by average of year beginning and year end assets\"\"\"\n",
    "\n",
    "        assets = AnnualAvg(inputs=[Fundamentals.total_assets],\n",
    "                           mask=mask)\n",
    "        sales = AnnualizedData([Fundamentals.total_revenue_asof_date,\n",
    "                                Fundamentals.total_revenue], mask=mask)\n",
    "        return sales / assets\n",
    "  \n",
    "    @staticmethod\n",
    "    def CurrentRatio(mask):\n",
    "        \"\"\"Total current assets divided by total current liabilities\"\"\"\n",
    "\n",
    "        assets = Fundamentals.current_assets.latest\n",
    "        liabilities = Fundamentals.current_liabilities.latest\n",
    "        return assets / liabilities\n",
    "    \n",
    "    @staticmethod\n",
    "    def AssetToEquityRatio(mask):\n",
    "        \"\"\"Total current assets divided by common equity\"\"\"\n",
    "\n",
    "        assets = Fundamentals.current_assets.latest\n",
    "        equity = Fundamentals.common_stock.latest\n",
    "        return assets / equity    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def InterestCoverage(mask):\n",
    "        \"\"\"EBIT divided by interest expense\"\"\"\n",
    "\n",
    "        ebit = AnnualizedData(inputs = [Fundamentals.ebit_asof_date,\n",
    "                                        Fundamentals.ebit], mask=mask)  \n",
    "        \n",
    "        interest_expense = AnnualizedData(inputs = [Fundamentals.interest_expense_asof_date,\n",
    "                                        Fundamentals.interest_expense], mask=mask)\n",
    "        return ebit / interest_expense\n",
    "\n",
    "    @staticmethod\n",
    "    def DebtToAssetRatio(mask):\n",
    "        \"\"\"Total Debts divided by Total Assets\"\"\"\n",
    "\n",
    "        debt = Fundamentals.total_debt.latest\n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return debt / assets\n",
    "    \n",
    "    @staticmethod\n",
    "    def DebtToEquityRatio(mask):\n",
    "        \"\"\"Total Debts divided by Common Stock Equity\"\"\"\n",
    "\n",
    "        debt = Fundamentals.total_debt.latest\n",
    "        equity = Fundamentals.common_stock.latest\n",
    "        return debt / equity    \n",
    "\n",
    "    @staticmethod\n",
    "    def WorkingCapitalToAssets(mask):\n",
    "        \"\"\"Current Assets less Current liabilities (Working Capital) divided by Assets\"\"\"\n",
    "\n",
    "        working_capital = Fundamentals.working_capital.latest\n",
    "        assets = Fundamentals.total_assets.latest\n",
    "        return working_capital / assets\n",
    " \n",
    "    @staticmethod\n",
    "    def WorkingCapitalToSales(mask):\n",
    "        \"\"\"Current Assets less Current liabilities (Working Capital), divided by Sales\"\"\"\n",
    "\n",
    "        working_capital = Fundamentals.working_capital.latest\n",
    "        sales = AnnualizedData([Fundamentals.total_revenue_asof_date,\n",
    "                                Fundamentals.total_revenue], mask=mask)        \n",
    "        return working_capital / sales          \n",
    "       \n",
    "        \n",
    "    class MertonsDD(CustomFactor):\n",
    "        \"\"\"Merton's Distance to Default \"\"\"\n",
    "        \n",
    "        inputs = [Fundamentals.total_assets,\n",
    "                  Fundamentals.total_liabilities, \n",
    "                  libor.value, \n",
    "                  USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, tot_assets, tot_liabilities, r, close):\n",
    "            mertons = []\n",
    "\n",
    "            for col_assets, col_liabilities, col_r, col_close in zip(tot_assets.T, tot_liabilities.T,\n",
    "                                                                     r.T, close.T):\n",
    "                vol_1y = np.nanstd(col_close)\n",
    "                numerator = np.log(\n",
    "                        col_assets[-1] / col_liabilities[-1]) + ((252 * col_r[-1]) - ((vol_1y ** 2) / 2))\n",
    "                mertons.append(numerator / vol_1y)\n",
    "\n",
    "            out[:] = mertons            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QUALITY_FACTORS = {\n",
    "    'AssetToEquityRatio'    : QualityFactors.AssetToEquityRatio,\n",
    "    'AssetTurnover'         : QualityFactors.AssetTurnover,\n",
    "    'CurrentRatio'          : QualityFactors.CurrentRatio,\n",
    "    'DebtToAssetRatio'      : QualityFactors.DebtToAssetRatio,\n",
    "    'DebtToEquityRatio'     : QualityFactors.DebtToEquityRatio,\n",
    "    'InterestCoverage'      : QualityFactors.InterestCoverage,\n",
    "    'MertonsDD'             : QualityFactors.MertonsDD,\n",
    "    'WorkingCapitalToAssets': QualityFactors.WorkingCapitalToAssets,\n",
    "    'WorkingCapitalToSales' : QualityFactors.WorkingCapitalToSales,\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_factors, t = factor_pipeline(QUALITY_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "quality_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payout Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PayoutFactors:\n",
    "\n",
    "    @staticmethod\n",
    "    def DividendPayoutRatio(mask):\n",
    "        \"\"\"Dividends Per Share divided by Earnings Per Share\"\"\"\n",
    "\n",
    "        dps = AnnualizedData(inputs = [Fundamentals.dividend_per_share_earnings_reports_asof_date,\n",
    "                                        Fundamentals.dividend_per_share_earnings_reports], mask=mask)  \n",
    "        \n",
    "        eps = AnnualizedData(inputs = [Fundamentals.basic_eps_earnings_reports_asof_date,\n",
    "                                        Fundamentals.basic_eps_earnings_reports], mask=mask)\n",
    "        return dps / eps\n",
    "    \n",
    "    @staticmethod\n",
    "    def DividendGrowth(**kwargs):\n",
    "        \"\"\"Annualized percentage DPS change\"\"\"        \n",
    "        return Fundamentals.dps_growth.latest    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAYOUT_FACTORS = {\n",
    "    'Dividend Payout Ratio': PayoutFactors.DividendPayoutRatio,\n",
    "    'Dividend Growth': PayoutFactors.DividendGrowth\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payout_factors, t = factor_pipeline(PAYOUT_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "payout_factors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profitability Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfitabilityFactors:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GrossProfitMargin(mask):\n",
    "        \"\"\"Gross Profit divided by Net Sales\"\"\"\n",
    "\n",
    "        gross_profit = AnnualizedData([Fundamentals.gross_profit_asof_date,\n",
    "                              Fundamentals.gross_profit], mask=mask)  \n",
    "        sales = AnnualizedData([Fundamentals.total_revenue_asof_date,\n",
    "                                Fundamentals.total_revenue], mask=mask)\n",
    "        return gross_profit / sales   \n",
    "    \n",
    "    @staticmethod\n",
    "    def NetIncomeMargin(mask):\n",
    "        \"\"\"Net income divided by Net Sales\"\"\"\n",
    "\n",
    "        net_income = AnnualizedData([Fundamentals.net_income_income_statement_asof_date,\n",
    "                              Fundamentals.net_income_income_statement], mask=mask)  \n",
    "        sales = AnnualizedData([Fundamentals.total_revenue_asof_date,\n",
    "                                Fundamentals.total_revenue], mask=mask)\n",
    "        return net_income / sales   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFITABIILTY_FACTORS = {\n",
    "    'Gross Profit Margin': ProfitabilityFactors.GrossProfitMargin,\n",
    "    'Net Income Margin': ProfitabilityFactors.NetIncomeMargin,\n",
    "    'Return on Equity': Fundamentals.roe.latest,\n",
    "    'Return on Assets': Fundamentals.roa.latest,\n",
    "    'Return on Invested Capital': Fundamentals.roic.latest\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability_factors, t = factor_pipeline(PAYOUT_FACTORS)\n",
    "print('Pipeline run time {:.2f} secs'.format(t))\n",
    "payout_factors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profitability_pipeline().show_graph(format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test predictions for various lookahead periods to identify the best holding periods that generate the best predictability, measured by the information coefficient. \n",
    "\n",
    "More specifically, we compute returns for 1, 5, 10, and 20 days using the built-in Returns function, resulting in over 50,000 observations for the universe of 100 stocks over two years (that include approximately 252 trading days each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead = [1, 5, 10, 20]\n",
    "returns = run_pipeline(Pipeline({'Returns{}D'.format(i): Returns(inputs=[USEquityPricing.close], \n",
    "                                          window_length=i+1, mask=UNIVERSE) for i in lookahead},\n",
    "                                screen=UNIVERSE),\n",
    "                       start_date=START, \n",
    "                       end_date=END)\n",
    "return_cols = ['Returns{}D'.format(i) for i in lookahead]\n",
    "returns.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use over 50 features that cover a broad range of factors based on market, fundamental, and alternative data. The notebook also includes custom transformations to convert fundamental data that is typically available in quarterly reporting frequency to rolling annual totals or averages to avoid excessive season fluctuations.\n",
    "\n",
    "Once the factors have been computed through the various pipelines outlined in Chapter 4, Alpha Factors – Research and Evaluation, we combine them using pd.concat(), assign index names, and create a categorical variable that identifies the asset for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([returns,\n",
    "                 value_factors,\n",
    "                 momentum_factors,\n",
    "                 quality_factors,\n",
    "                 payout_factors,\n",
    "                 growth_factors,\n",
    "                 efficiency_factors,\n",
    "                 risk_factors], axis=1).sortlevel()\n",
    "data.index.names = ['date', 'asset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stock'] = data.index.get_level_values('asset').map(lambda x: x.asset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns and rows with less than 80% of data availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we remove rows and columns that lack more than 20 percent of the observations, resulting in a loss of six percent of the observations and three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before, cols_before = data.shape\n",
    "data = (data\n",
    "        .dropna(axis=1, thresh=int(len(data)*.8))\n",
    "        .dropna(thresh=int(len(data.columns) * .8)))\n",
    "data = data.fillna(data.median())\n",
    "rows_after, cols_after = data.shape\n",
    "print('{:,d} rows and {:,d} columns dropped'.format(rows_before-rows_after, cols_before-cols_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have 51 features and the categorical identifier of the stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_index(1).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression models, it is important to explore the correlation among the features to identify multicollinearity issues, and to check the correlation between the features and the target. The notebook contains a seaborn clustermap that shows the hierarchical structure of the feature correlation matrix. It identifies a small number of highly correlated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(data.drop(['stock'] + return_cols, axis=1).corr())\n",
    "plt.gcf().set_size_inches((14,14));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the categorical stock variable into a numeric format so that the linear regression can process it. For this purpose, we use dummy encoding that creates individual columns for each category level and flags the presence of this level in the original categorical column with an entry of 1, and 0 otherwise. The pandas function get_dummies() automates dummy encoding. It detects and properly converts columns of type objects as illustrated next. If you need dummy variables for columns containing integers, for instance, you can identify them using the keyword columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(data.drop(return_cols, axis=1))\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating forward returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict returns over a given holding period. Hence, we need to align the features with return values with the corresponding return data point 1, 5, 10, or 20 days into the future for each equity. We achieve this by combining the pandas .groupby() method with the .shift() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = data.loc[:, return_cols]\n",
    "shifted_y = []\n",
    "for col in y.columns:\n",
    "    t = int(re.search(r'\\d+', col).group(0))\n",
    "    shifted_y.append(y.groupby(level='asset')['Returns{}D'.format(t)].shift(-t).to_frame(col))\n",
    "y = pd.concat(shifted_y, axis=1)\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(y[return_cols])\n",
    "ax.set_title('Return Distriubtions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate a linear regression model using OLS with statsmodels as demonstrated previously. We select a forward return, for example for a 10-day holding period, remove outliers below the 2.5% and above the 97.5% percentiles, and fit the model accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'Returns1D'\n",
    "model_data = pd.concat([y[[target]], X], axis=1).dropna()\n",
    "model_data = model_data[model_data[target].between(model_data[target].quantile(.025), \n",
    "                                                   model_data[target].quantile(.975))]\n",
    "\n",
    "model = OLS(endog=model_data[target], exog=model_data.drop(target, axis=1))\n",
    "trained_model = model.fit()\n",
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary is available in the notebook to save some space due to the large number of variables. The diagnostic statistics show that, given the high p-value on the Jarque—Bera statistic, the hypothesis that the residuals are normally distributed cannot be rejected.\n",
    "\n",
    "However, the Durbin—Watson statistic is low at 1.5 so we can reject the null hypothesis of no autocorrelation comfortably at the 5% level. Hence, the standard errors are likely positively correlated. If our goal were to understand which factors are significantly associated with forward returns, we would need to rerun the regression using robust standard errors (a parameter in statsmodels .fit() method), or use a different method altogether such as a panel model that allows for more complex error covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Returns5D'\n",
    "model_data = pd.concat([y[[target]], X], axis=1).dropna()\n",
    "model_data = model_data[model_data[target].between(model_data[target].quantile(.025), \n",
    "                                                   model_data[target].quantile(.975))]\n",
    "\n",
    "model = OLS(endog=model_data[target], exog=model_data.drop(target, axis=1))\n",
    "trained_model = model.fit()\n",
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Returns10D'\n",
    "model_data = pd.concat([y[[target]], X], axis=1).dropna()\n",
    "model_data = model_data[model_data[target].between(model_data[target].quantile(.025), \n",
    "                                                   model_data[target].quantile(.975))]\n",
    "\n",
    "model = OLS(endog=model_data[target], exog=model_data.drop(target, axis=1))\n",
    "trained_model = model.fit()\n",
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Returns20D'\n",
    "model_data = pd.concat([y[[target]], X], axis=1).dropna()\n",
    "model_data = model_data[model_data[target].between(model_data[target].quantile(.025), \n",
    "                                                   model_data[target].quantile(.975))]\n",
    "\n",
    "model = OLS(endog=model_data[target], exog=model_data.drop(target, axis=1))\n",
    "trained_model = model.fit()\n",
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models for Prediction: sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sklearn is tailored towards prediction, we will evaluate the linear regression model based on its predictive performance using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of grouped time series data that requires a custom cross-validation function to provide the train and test indices that ensure that the test data immediately follows the training data for each equity and we do not inadvertently create a look-ahead bias or leakage.\n",
    "\n",
    "We can achieve this using the following function that returns a generator yielding pairs of train and test dates. The set of train dates that ensure a minimum length of the training periods. The number of pairs depends on the parameter nfolds. The distinct test periods do not overlap and are located at the end of the period available in the data. After a test period is used, it becomes part of the training data that grow in size accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_split(d=model_data, nfolds=5, min_train=21):\n",
    "    \"\"\"Generate train/test dates for nfolds \n",
    "    with at least min_train train obs\n",
    "    \"\"\"\n",
    "    train_dates = d[:min_train].tolist()\n",
    "    n = int(len(dates)/(nfolds + 1)) + 1\n",
    "    test_folds = [d[i:i + n] for i in range(min_train, len(d), n)]\n",
    "    for test_dates in test_folds:\n",
    "        if len(train_dates) > min_train:\n",
    "            yield train_dates, test_dates\n",
    "        train_dates.extend(test_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select the appropriate return series (we will again use a 10-day holding period) and remove outliers. We will also convert returns to log returns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Returns10D'\n",
    "outliers = .01\n",
    "model_data = pd.concat([y[[target]], X], axis=1).dropna().reset_index('asset', drop=True)\n",
    "model_data = model_data[model_data[target].between(*model_data[target].quantile([outliers, 1-outliers]).values)] \n",
    "\n",
    "model_data[target] = np.log1p(model_data[target])\n",
    "features = model_data.drop(target, axis=1).columns\n",
    "dates = model_data.index.unique()\n",
    "\n",
    "print(model_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[target].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 250 folds to generally predict about 2 days of forward returns following the historical training data that will gradually increase in length. \n",
    "\n",
    "Each iteration obtains the appropriate training and test dates from our custom cross-validation function, selects the corresponding features and targets, and then trains and predicts accordingly. \n",
    "\n",
    "We capture the root mean squared error as well as the Spearman rank correlation between actual and predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 250\n",
    "lr = LinearRegression()\n",
    "\n",
    "test_results, result_idx, preds = [], [], pd.DataFrame()\n",
    "for train_dates, test_dates in time_series_split(dates, nfolds=nfolds):\n",
    "    \n",
    "    X_train = model_data.loc[idx[train_dates], features]\n",
    "    y_train = model_data.loc[idx[train_dates], target]\n",
    "    lr.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    X_test = model_data.loc[idx[test_dates], features]\n",
    "    y_test = model_data.loc[idx[test_dates], target]\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n",
    "    ic, pval = spearmanr(y_pred, y_test)\n",
    "    \n",
    "    test_results.append([rmse, ic, pval])\n",
    "    preds = preds.append(y_test.to_frame('actuals').assign(predicted=y_pred))\n",
    "    result_idx.append(train_dates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame(test_results, columns=['rmse', 'ic', 'pval'], index=result_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have captured the test predictions from the 250 folds and can compute both the overall and a 21-day rolling average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2)\n",
    "rolling_result = test_result.rolling(21).mean()\n",
    "rolling_result[['ic', 'pval']].plot(ax=axes[0], title='Information Coefficient')\n",
    "axes[0].axhline(test_result.ic.mean(), lw=1, ls='--', color='k')\n",
    "rolling_result[['rmse']].plot(ax=axes[1], title='Root Mean Squared Error')\n",
    "axes[1].axhline(test_result.rmse.mean(), lw=1, ls='--', color='k')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entire period, we see that the Information Coefficient measured by the rank correlation of actual and predicted returns is weakly positive and statistically significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cleaned = preds[(preds.predicted.between(*preds.predicted.quantile([.001, .999]).values))]\n",
    "sns.jointplot(x='actuals', y='predicted', data=preds_cleaned, stat_func=spearmanr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ridge regression, we need to tune the regularization parameter with the keyword alpha that corresponds to the λ we used previously. We will try 21 values from 10-5 to 105 in logarithmic steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Ridge Regression: L2 Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale sensitivity of the ridge penalty requires us to standardize the inputs using the StandardScaler. Note that we always learn the mean and the standard deviation from the training set using the .fit_transform() method and then apply these learned parameters to the test set using the .transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nfolds = 250\n",
    "alphas = np.logspace(-5, 5, 11)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "ridge_result, ridge_coeffs = pd.DataFrame(), pd.DataFrame()\n",
    "for i, alpha in enumerate(alphas):\n",
    "    print i, \n",
    "    coeffs, test_results = [], []\n",
    "    lr_ridge = Ridge(alpha=alpha)\n",
    "    for train_dates, test_dates in time_series_split(dates, nfolds=nfolds):\n",
    "\n",
    "        X_train = model_data.loc[idx[train_dates], features]\n",
    "        y_train = model_data.loc[idx[train_dates], target]\n",
    "        lr_ridge.fit(X=scaler.fit_transform(X_train), y=y_train)\n",
    "        coeffs.append(lr_ridge.coef_)\n",
    "\n",
    "        X_test = model_data.loc[idx[test_dates], features]\n",
    "        y_test = model_data.loc[idx[test_dates], target]\n",
    "        y_pred = lr_ridge.predict(scaler.transform(X_test))\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n",
    "        ic, pval = spearmanr(y_pred, y_test)\n",
    "        \n",
    "        test_results.append([train_dates[-1], rmse, ic, pval, alpha])\n",
    "    test_results = pd.DataFrame(test_results, columns=['date', 'rmse', 'ic', 'pval', 'alpha'])\n",
    "    ridge_result = ridge_result.append(test_results)\n",
    "    ridge_coeffs[alpha] = np.mean(coeffs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of Information Coefficients - p-value Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ridge_result.pval, bins=20, norm_hist=True, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_result_sig = ridge_result[(ridge_result.pval < .05) & (ridge_result.alpha.between(10**-5, 10**5))]\n",
    "ridge_result_sig_alpha = ridge_result_sig.groupby('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coeffs_main = ridge_coeffs.filter(ridge_result_sig.alpha.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the information coefficient obtained for each hyperparameter value and also visualize how the coefficient values evolve as the regularization increases. The results show that we get the highest IC value for a value of λ=10. For this level of regularization, the right-hand panel reveals that the coefficients have been already significantly shrunk compared to the (almost) unconstrained model with λ=10-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ic = ridge_result_sig_alpha['ic'].mean().max()\n",
    "best_alpha = ridge_result_sig_alpha['ic'].mean().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, sharex=True)\n",
    "\n",
    "ridge_result.groupby('alpha')['ic'].mean().plot(logx=True, title='Information Coefficient', ax=axes[0])\n",
    "axes[0].axhline(ridge_result.groupby('alpha').ic.mean().median())\n",
    "axes[0].axvline(x=ridge_result.groupby('alpha').ic.mean().idxmax(), c='darkgrey', ls='--')\n",
    "axes[0].set_xlabel('Regularization')\n",
    "axes[0].set_ylabel('Information Coefficient')\n",
    "\n",
    "ridge_coeffs_main.T.plot(legend=False, logx=True, title='Ridge Path', ax=axes[1])\n",
    "axes[1].set_xlabel('Regularization')\n",
    "axes[1].set_ylabel('Coefficients')\n",
    "axes[1].axvline(x=ridge_result.groupby('alpha').ic.mean().idxmax(), c='darkgrey', ls='--')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standardization of the coefficients allows us to draw conclusions about their relative importance by comparing their absolute magnitude. The 10 most relevant coefficients are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coeffs = ridge_coeffs_main.loc[:, best_alpha]\n",
    "model_coeffs.index = features\n",
    "model_coeffs.abs().sort_values().tail(10).plot.barh(title='Top 10 Factors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Result Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(y='ic', x='alpha', data=ridge_result_sig)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso implementation looks very similar to the ridge model we just ran. The main difference is that lasso needs to arrive at a solution using iterative coordinate descent whereas ridge can rely on a closed-form solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 250\n",
    "alphas = np.logspace(-8, -2, 13)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lasso_results, lasso_coeffs = pd.DataFrame(), pd.DataFrame()\n",
    "for i, alpha in enumerate(alphas):\n",
    "    print i,\n",
    "    coeffs, test_results = [], []\n",
    "    lr_lasso = Lasso(alpha=alpha)\n",
    "    for i, (train_dates, test_dates) in enumerate(time_series_split(dates, nfolds=nfolds)):\n",
    "        X_train = model_data.loc[idx[train_dates], features]\n",
    "        y_train = model_data.loc[idx[train_dates], target]\n",
    "        lr_lasso.fit(X=scaler.fit_transform(X_train), y=y_train)\n",
    "        \n",
    "        X_test = model_data.loc[idx[test_dates], features]\n",
    "        y_test = model_data.loc[idx[test_dates], target]\n",
    "        y_pred = lr_lasso.predict(scaler.transform(X_test))\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n",
    "        ic, pval = spearmanr(y_pred, y_test)\n",
    "        \n",
    "        coeffs.append(lr_lasso.coef_)\n",
    "        test_results.append([train_dates[-1], rmse, ic, pval, alpha])\n",
    "    test_results = pd.DataFrame(test_results, columns=['date', 'rmse', 'ic', 'pval', 'alpha'])\n",
    "    lasso_results = lasso_results.append(test_results)\n",
    "    lasso_coeffs[alpha] = np.mean(coeffs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_results.groupby('alpha').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(y='ic', x='alpha', data=lasso_results)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated information coefficient and Lasso Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can plot the average information coefficient for all test sets used during cross-validation. We see again that regularization improves the IC over the unconstrained model, delivering the best out-of-sample result at a level of λ=10-5. The optimal regularization value is quite different from ridge regression because the penalty consists of the sum of the absolute, not the squared values of the relatively small coefficient values. We can also see that for this regularization level, the coefficients have been similarly shrunk, as in the ridge regression case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, sharex=True)\n",
    "\n",
    "lasso_results.groupby('alpha')['ic'].mean().plot(logx=True, title='Information Coefficient', ax=axes[0])\n",
    "axes[0].axhline(lasso_results.groupby('alpha')['ic'].mean().median())\n",
    "axes[0].axvline(x=lasso_results.groupby('alpha')['ic'].mean().idxmax(), c='darkgrey', ls='--')\n",
    "axes[0].set_xlabel('Regularization')\n",
    "axes[0].set_ylabel('Information Coefficient')\n",
    "\n",
    "lasso_coeffs.T.plot(legend=False, logx=True, title='Lasso Path', ax=axes[1])\n",
    "axes[1].set_xlabel('Regularization')\n",
    "axes[1].set_ylabel('Coefficients')\n",
    "axes[1].axvline(x=lasso_results.groupby('alpha')['ic'].mean().idxmax(), c='darkgrey', ls='--')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, ridge and lasso will produce similar results. Ridge often computes faster, but lasso also yields continuous features subset selection by gradually reducing coefficients to zero, hence eliminating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
